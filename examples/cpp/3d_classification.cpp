/// Example of using OctNet for denoising using a single convolutional layer
/// as auto-encoder.
///
/// @author David Stutz
/// @file 3d_auto_encoder.cpp

#define N_THREADS 4

#include <cstdlib>
#include <iostream>
#include <vector>
#include <cassert>
#include <cmath>
#include "octnet/core/types.h"
#include "octnet/core/core.h"
#include "octnet/cpu/cpu.h"
#include "octnet/cpu/dense.h"
#include "octnet/cpu/combine.h"
#include "octnet/cpu/conv.h"
#include "octnet/cpu/activations.h"
#include "octnet/cpu/loss.h"
#include "octnet/cpu/pool.h"
#include "octnet/cpu/fc.h"
#include "octnet/create/create.h"

/// Create a cube with the given label, i.e. the label determines whether the
/// cube is in the upper or lower half (along the height axis).
/// 
/// @param data
/// @param depth
/// @param height
/// @param width
/// @param label
void create_cube(ot_data_t* data, int depth, int height, int width, ot_data_t label) {
  
  int size_d = 4 + std::rand()%3;
  int size_h = 4 + std::rand()%3;
  int size_w = 4 + std::rand()%3;
  
  int anchor_d = std::rand()%(depth - size_d);
  int anchor_w = std::rand()%(width - size_w);
  
  int anchor_h = std::rand()%(height/2 - size_h);
  if (label > 0) {
    anchor_h += height/2;
  }
  
  for (int d = anchor_d; d < anchor_d + size_d; d++) {
    for (int h = anchor_h; h < anchor_h + size_h; h++) {
      for (int w = anchor_w; w < anchor_w + size_w; w++) {
        data[(d*height + h)*width + w] = 1.f;
      }
    }
  }
}

/// Initialize an array with zero.
/// 
/// @param array
/// @param n
void initialize_zero(ot_data_t* array, int n) {
  for (int i = 0; i < n; i++) {
    array[i] = 0;
  }
}

/// Create a simple dataset of the given sizes.
/// 
/// @param dataset
/// @param N
/// @param depth
/// @param height
/// @param width
/// @param labels
void create_dataset(ot_data_t* dataset, int N, int depth, int height, int width, ot_data_t* labels) {
  
  initialize_zero(dataset, N*depth*height*width);
  for (int n = 0; n < N; n++) {
    
    float r = std::rand() / ((float) RAND_MAX);
    if (r > 0.5) {
      labels[n] = 1.f;
    }
    else {
      labels[n] = 0.f;
    }
    
    create_cube(dataset + n*height*depth*width, depth, height, width, labels[n]);
  }
}

/// Initialize the weights for convolutional layers.
/// 
/// @param weights
/// @param n
/// @param neurons_in
/// @param neurons_out
void initialize_weights(ot_data_t* weights, int n, int neurons_in, int neurons_out) {
  for (int i = 0; i < n; i++) {
    // https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform
    ot_data_t u_1 = std::rand() / ((float) RAND_MAX);
    ot_data_t u_2 = std::rand() / ((float) RAND_MAX);
    ot_data_t g = std::sqrt(-2*std::log(u_1))*std::cos(2*M_PI*u_2);
    
    // xavier initialization
    weights[i] = g*(2.f/(neurons_in + neurons_out));
  }
}

/// Initialize biases.
/// 
/// @param biases
/// @param n
void initialize_biases(ot_data_t* biases, int n) {
  for (int i = 0; i < n; i++) {
    biases[i] = 0.5f;
  }
}

/// Compute magnitude of weight array.
///
/// @param array
/// @param n
/// @param scale
/// @return 
ot_data_t compute_magnitude(ot_data_t* array, int n, ot_data_t scale = 1.f) {
  ot_data_t mag = 0.f;
  for (int i = 0; i < n; i++) {
    mag += (1.f/scale)*(1.f/scale)*array[i]*array[i];
  }
  return std::sqrt(mag);
}

/// Clip gradients by factor (multiplication).
/// 
/// @param array
/// @param n
/// @param factor
void clip_gradients(ot_data_t* array, int n, ot_data_t factor) {
  for (int i = 0; i < n; i++) {
    array[i] *= factor;
  }
}

/// Auto-encoder training for denoising 8x8x8 volumes.
///
/// @param argc
/// @param argv
/// @return
int main(int argc, char** argv) {

  // note: the below parameters doe currently work (at least when I tried it last).
  // the problem seems to be very sensitive to the parameters and the initialization
  // there is also still some oscillation going on I didn't get rid of and
  // convergence is pretty slow
  
  const int N = 1000; ///< number of training samples
  const int depth = 16;
  const int height = 16;
  const int width = 16;
  const int T = 250; ///< number of iterations
  const ot_data_t gradient_clipping = 1e18; ///< clip gradients if the magnitude exceeds this number
  const ot_data_t scale = 1.f; // scale for conv backward passes
  const int conv1_channels = 8; ///< numbe rof channels generated by conv3 layer
  const int conv2_channels = 8; ///< number of channels generated by conv2 layer
  const int batch_size = 16; ///< batch size to use during training
  ot_data_t learning_rate = 0.005f; ///< learning rate for learning
  ot_data_t momentum = 0.9f; ///< momentum term for learning
  
  // for octree conversion ...
  const int n_ranges = 1;
  ot_data_t* ranges = new ot_data_t[2];
  ranges[0] = 0.5f;
  ranges[1] = 1.5f;
  
  ot_data_t* dataset = new ot_data_t[N*depth*height*width];
  ot_data_t* labels = new ot_data_t[N];
  create_dataset(dataset, N, depth, height, width, labels);
  
  // weights for conv1
  const int conv1_weights_size = conv1_channels*1*3*3*3;
  ot_data_t* conv1_weights = new ot_data_t[conv1_weights_size];
  initialize_weights(conv1_weights, conv1_weights_size, 3*3*3, conv1_weights_size);
  ot_data_t* conv1_biases = new ot_data_t[conv1_channels];
  initialize_biases(conv1_biases, conv1_channels);
  
  // gradients for conv1 (are initialized every iteration)
  ot_data_t* grad_conv1_weights = new ot_data_t[conv1_weights_size];
  ot_data_t* grad_conv1_biases = new ot_data_t[conv1_channels];
  
  // momentum for conv1
  ot_data_t* momentum_conv1_weights = new ot_data_t[conv1_weights_size];
  initialize_zero(momentum_conv1_weights, conv1_weights_size);
  ot_data_t* momentum_conv1_biases = new ot_data_t[conv1_channels];
  initialize_zero(momentum_conv1_biases, conv1_channels);
  
  // weights for conv2
  const int conv2_weights_size = conv2_channels*conv1_channels*3*3*3;
  ot_data_t* conv2_weights = new ot_data_t[conv2_weights_size];
  initialize_weights(conv2_weights, conv2_weights_size, conv1_weights_size, conv2_weights_size);
  ot_data_t* conv2_biases = new ot_data_t[conv2_channels];
  initialize_biases(conv2_biases, conv2_channels);
  
  // gradients for conv2 (are initialized every iteration)
  ot_data_t* grad_conv2_weights = new ot_data_t[conv2_weights_size];
  ot_data_t* grad_conv2_biases = new ot_data_t[conv2_channels];
  
  // momentum for conv2
  ot_data_t* momentum_conv2_weights = new ot_data_t[conv2_weights_size];
  initialize_zero(momentum_conv2_weights, conv2_weights_size);
  ot_data_t* momentum_conv2_biases = new ot_data_t[conv2_channels];
  initialize_zero(momentum_conv2_biases, conv2_channels);
  
  // weights for fc3
  const int fc3_num_input = conv2_channels*8*8*8;
  const int fc3_num_output = 1;
  ot_data_t* fc3_weights = new ot_data_t[fc3_num_input*fc3_num_output];
  initialize_weights(fc3_weights, fc3_num_input*fc3_num_output, fc3_num_input, fc3_num_output);
  ot_data_t* fc3_biases = new ot_data_t[fc3_num_output];
  initialize_zero(fc3_biases, fc3_num_output);
  
  // gradients for fc3
  ot_data_t* grad_fc3_weights = new ot_data_t[fc3_num_input*fc3_num_output];
  ot_data_t* grad_fc3_biases = new ot_data_t[fc3_num_output];
  
  // momentum for fc3
  ot_data_t* momentum_fc3_weights = new ot_data_t[fc3_num_input*fc3_num_output];
  initialize_zero(momentum_fc3_weights, fc3_num_input*fc3_num_output);
  ot_data_t* momentum_fc3_biases = new ot_data_t[fc3_num_output];
  initialize_zero(momentum_fc3_biases, fc3_num_output);
  
  // these will be intermediate octrees and arrays that are reused
  octree* layer_conv1 = octree_new_cpu();
  octree* layer_sigmoid1 = octree_new_cpu();
  octree* layer_conv2 = octree_new_cpu();
  octree* layer_sigmoid2 = octree_new_cpu();
  octree* layer_pool2 = octree_new_cpu();
  
  // for fully connected layers, the batch sizes needs to be considered
  // explicitly!
  ot_data_t* layer_dense3 = new ot_data_t[batch_size*fc3_num_input];
  ot_data_t* layer_fc3 = new ot_data_t[batch_size*fc3_num_output];
  ot_data_t* layer_sigmoid3 = new ot_data_t[batch_size*fc3_num_output];
  
  ot_data_t* grad_loss = new ot_data_t[batch_size*fc3_num_output];
  ot_data_t* grad_sigmoid3 = new ot_data_t[batch_size*fc3_num_output];
  octree* grad_dense3 = octree_new_cpu();
  ot_data_t* grad_fc3 = new ot_data_t[batch_size*fc3_num_input];
  octree* grad_pool2 = octree_new_cpu();
  octree* grad_sigmoid2 = octree_new_cpu();
  octree* grad_conv2 = octree_new_cpu();
  octree* grad_sigmoid1 = octree_new_cpu();
  octree* grad_conv1 = octree_new_cpu();
  
  for (int t, tt = 0; t < T; t++, tt++) {
    int b = std::rand()%(N - batch_size);
    
    // create a batch of the set size by combining octrees (not completely
    // randomly selected yet)
    //octree* batch = octree_new_cpu();
    //octree_combine_n_cpu(&octrees[b], batch_size, batch);
    octree* batch = octree_create_from_dense_batch_cpu(dataset + b*height*width*depth, batch_size, depth, height, width, n_ranges, ranges, false, 0, false, N_THREADS);
            
    // do the forward pass
    octree_conv3x3x3_avg_cpu(batch, conv1_weights, conv1_biases, conv1_channels, layer_conv1); octree_check_nan_inf_cpu(layer_conv1, "layer_conv1");
    octree_sigmoid_cpu(layer_conv1, false, layer_sigmoid1); octree_check_nan_inf_cpu(layer_sigmoid1, "layer_sigmoid1");
    
    octree_conv3x3x3_avg_cpu(layer_sigmoid1, conv2_weights, conv2_biases, conv2_channels, layer_conv2); octree_check_nan_inf_cpu(layer_conv2, "layer_conv2");
    octree_sigmoid_cpu(layer_conv2, false, layer_sigmoid2); octree_check_nan_inf_cpu(layer_sigmoid2, "layer_sigmoid2");
    octree_gridpool2x2x2_max_cpu(layer_sigmoid2, layer_pool2); octree_check_nan_inf_cpu(layer_pool2, "layer_pool2");
    
    octree_to_dhwc_cpu(layer_pool2, 8, 8, 8, layer_dense3); dense_check_nan_inf_cpu(layer_dense3, batch_size*fc3_num_input, "layer_dense3");
    dense_fc_cpu(layer_dense3, fc3_weights, fc3_biases, batch_size, fc3_num_input, fc3_num_output, layer_fc3); dense_check_nan_inf_cpu(layer_fc3, batch_size*fc3_num_output, "layer_fc3");
    
//    std::cout << "0 -- label " << labels[b] << std::endl;
//    octree_print_cpu(octrees[b]);
//    for (int j = 0; j < fc3_num_input; j++) {
//      std::cout << layer_dense3[0*fc3_num_input + j] << " ";
//    }
//    std::cout << std::endl;
//    
//    std::cout << "1 -- label " << labels[b + 1] << std::endl;
//    octree_print_cpu(octrees[b + 1]);
//    for (int j = 0; j < fc3_num_input; j++) {
//      std::cout << layer_dense3[1*fc3_num_input + j] << " ";
//    }
//    std::cout << std::endl;
    
    dense_sigmoid_cpu(layer_fc3, batch_size, fc3_num_output, layer_sigmoid3); dense_check_nan_inf_cpu(layer_sigmoid3, batch_size*fc3_num_output, "layer_sigmoid3");
    ot_data_t error = dense_bce_cpu(layer_sigmoid3, labels + b, batch_size, fc3_num_output);
    
    // seems like the gradient arrays need to be initialized every time!
    initialize_zero(grad_conv1_weights, conv1_weights_size);
    initialize_zero(grad_conv1_biases, conv1_channels);
    
    initialize_zero(grad_conv2_weights, conv2_weights_size);
    initialize_zero(grad_conv2_biases, conv2_channels);
    
    initialize_zero(grad_fc3_weights, fc3_num_input*fc3_num_output);
    initialize_zero(grad_fc3_biases, fc3_num_output);
    
    // do the backward pass
    dense_bce_bwd_cpu(layer_sigmoid3, labels + b, batch_size, fc3_num_output, grad_loss); dense_check_nan_inf_cpu(grad_loss, batch_size*fc3_num_output, "grad_loss");
    
    dense_sigmoid_bwd_cpu(layer_sigmoid3, grad_loss, batch_size, fc3_num_output, grad_sigmoid3); dense_check_nan_inf_cpu(grad_sigmoid3, batch_size*fc3_num_output, "grad_sigmoid3");
    dense_fc_bwd_cpu(fc3_weights, grad_sigmoid3, batch_size, fc3_num_input, fc3_num_output, grad_fc3); dense_check_nan_inf_cpu(grad_fc3, batch_size*fc3_num_input, "grad_fc3");
    dense_fc_wbwd_cpu(layer_dense3, grad_sigmoid3, batch_size, fc3_num_input, fc3_num_output, scale, grad_fc3_weights, grad_fc3_biases); 
    dense_check_nan_inf_cpu(grad_fc3_weights, fc3_num_input*fc3_num_output, "grad_fc3_weights"); dense_check_nan_inf_cpu(grad_fc3_biases, fc3_num_output, "grad_fc3_biases");
    
    octree_to_dhwc_bwd_cpu(layer_pool2, 8, 8, 8, grad_fc3, grad_dense3); octree_check_nan_inf_cpu(grad_dense3, "grad_dense3");
    octree_gridpool2x2x2_max_bwd_cpu(layer_sigmoid2, grad_dense3, grad_pool2); octree_check_nan_inf_cpu(grad_pool2, "grad_pool2");
    octree_sigmoid_bwd_cpu(layer_conv2, layer_sigmoid2, grad_pool2, false, grad_sigmoid2); octree_check_nan_inf_cpu(grad_sigmoid2, "grad_sigmoid2");
    octree_conv3x3x3_avg_bwd_cpu(conv2_weights, grad_sigmoid2, conv1_channels, grad_conv2); octree_check_nan_inf_cpu(grad_conv2, "grad_conv2");
    octree_conv3x3x3_avg_wbwd_cpu(layer_sigmoid1, grad_sigmoid2, 1.f, grad_conv2_weights, grad_conv2_biases);
    dense_check_nan_inf_cpu(grad_conv2_weights, conv2_weights_size, "grad_conv2_weights"); dense_check_nan_inf_cpu(grad_conv2_biases, conv2_channels, "grad_conv2_biases");
    
    octree_sigmoid_bwd_cpu(layer_conv1, layer_sigmoid1, grad_conv2, false, grad_sigmoid1); octree_check_nan_inf_cpu(grad_sigmoid1, "grad_sigmoid1");
    octree_conv3x3x3_avg_bwd_cpu(conv1_weights, grad_sigmoid1, 1, grad_conv1); octree_check_nan_inf_cpu(grad_conv1, "grad_conv1");
    octree_conv3x3x3_avg_wbwd_cpu(batch, grad_sigmoid1, 1.f, grad_conv1_weights, grad_conv1_biases);
    dense_check_nan_inf_cpu(grad_conv1_weights, conv1_weights_size, "grad_conv1_weights"); dense_check_nan_inf_cpu(grad_conv1_biases, conv1_channels, "grad_conv1_biases");
    
    ot_data_t grad_conv1_weights_mag = compute_magnitude(grad_conv1_weights, conv1_weights_size, scale);
    if (grad_conv1_weights_mag > gradient_clipping) {
      clip_gradients(grad_conv1_weights, conv1_weights_size, 1.f/grad_conv1_weights_mag*gradient_clipping);
    }
    
    ot_data_t grad_conv2_weights_mag = compute_magnitude(grad_conv2_weights, conv2_weights_size, scale);
    if (grad_conv2_weights_mag > gradient_clipping) {
      clip_gradients(grad_conv2_weights, conv2_weights_size, 1.f/grad_conv2_weights_mag*gradient_clipping);
    }
    
    ot_data_t grad_fc3_weights_mag = compute_magnitude(grad_fc3_weights, fc3_num_input*fc3_num_output, scale);
    if (grad_fc3_weights_mag > gradient_clipping) {
      clip_gradients(grad_fc3_weights, fc3_num_input*fc3_num_output, 1.f/grad_fc3_weights_mag*gradient_clipping);
    }
    
    for (int i = 0; i < conv1_weights_size; i++) {
      momentum_conv1_weights[i] = momentum*momentum_conv1_weights[i] - 1.f/scale*learning_rate*grad_conv1_weights[i];
      conv1_weights[i] += momentum_conv1_weights[i];
    }
    
    for (int i = 0; i < conv2_weights_size; i++) {
      momentum_conv2_weights[i] = momentum*momentum_conv2_weights[i] - 1.f/scale*learning_rate*grad_conv2_weights[i];
      conv2_weights[i] += momentum_conv2_weights[i];
    }
    
    for (int i = 0; i < fc3_num_input*fc3_num_output; i++) {
      momentum_fc3_weights[i] = momentum*momentum_fc3_weights[i] - 1.f/scale*learning_rate*grad_fc3_weights[i];
      fc3_weights[i] += momentum_fc3_weights[i];
    }
    
    // note that the scale does not influence the bias gradients!
    
    ot_data_t grad_conv1_biases_mag = compute_magnitude(grad_conv1_biases, conv1_channels, 1.f);
    if (grad_conv1_biases_mag > gradient_clipping) {
      clip_gradients(grad_conv1_biases, conv1_channels, 1.f/grad_conv1_biases_mag*gradient_clipping);
    }
    
    ot_data_t grad_conv2_biases_mag = compute_magnitude(grad_conv2_biases, conv2_channels, 1.f);
    if (grad_conv2_biases_mag > gradient_clipping) {
      clip_gradients(grad_conv2_biases, conv2_channels, 1.f/grad_conv2_biases_mag*gradient_clipping);
    }
    
    ot_data_t grad_fc3_biases_mag = compute_magnitude(grad_fc3_biases, fc3_num_output, 1.f);
    if (grad_fc3_biases_mag > gradient_clipping) {
      clip_gradients(grad_fc3_biases, fc3_num_output, 1.f/grad_fc3_biases_mag*gradient_clipping);
    }
    
    for (int i = 0; i < conv1_channels; i++) {
      momentum_conv1_biases[i] = momentum*momentum_conv1_biases[i] - learning_rate*grad_conv1_biases[i];
      conv1_biases[i] += momentum_conv1_biases[i];
    }
    
    for (int i = 0; i < conv2_channels; i++) {
      momentum_conv2_biases[i] = momentum*momentum_conv2_biases[i] - learning_rate*grad_conv2_biases[i];
      conv2_biases[i] += momentum_conv2_biases[i];
    }
    
    for (int i = 0; i < fc3_num_output; i++) {
      momentum_fc3_biases[i] = momentum*momentum_fc3_biases[i] - learning_rate*grad_fc3_biases[i];
      fc3_biases[i] += momentum_fc3_biases[i];
    }
    
    std::cout << "Predictions [" << t << "]: ";
    for (int i = 0; i < batch_size; i++) {
      std::cout << layer_sigmoid3[i] << "|" << (labels + b)[i] << " ";
    }
    std::cout << std::endl;
    
    std::cout << "Error [" << t << "]: " << error << std::endl;
    std::cout << "Gradient Magnitude [" << t << "]: " << grad_conv1_weights_mag
            << " | " << grad_conv2_weights_mag
            << " | " << grad_fc3_weights_mag << std::endl;
    
    octree_free_cpu(batch);
  }
  
  //for (int n = 0; n < N; n++) {
  //  octree_free_cpu(octrees[n]);
  //}
  
  octree_free_cpu(layer_conv1);
  octree_free_cpu(layer_sigmoid1);
  octree_free_cpu(layer_conv2);
  octree_free_cpu(layer_sigmoid2);
  octree_free_cpu(layer_pool2);
  delete[] layer_dense3;
  delete[] layer_fc3;
  delete[] layer_sigmoid3;
  
  delete[] grad_sigmoid3;
  delete[] grad_fc3;
  octree_free_cpu(grad_dense3);
  octree_free_cpu(grad_pool2);
  octree_free_cpu(grad_sigmoid2);
  octree_free_cpu(grad_conv2);
  octree_free_cpu(grad_sigmoid1);
  octree_free_cpu(grad_conv1);
  
  delete[] conv1_weights;
  delete[] conv1_biases;
  delete[] grad_conv1_weights;
  delete[] grad_conv1_biases;
  delete[] momentum_conv1_weights;
  delete[] momentum_conv1_biases;
  
  delete[] conv2_weights;
  delete[] conv2_biases;
  delete[] grad_conv2_weights;
  delete[] grad_conv2_biases;
  delete[] momentum_conv2_weights;
  delete[] momentum_conv2_biases;
  
  delete[] fc3_weights;
  delete[] fc3_biases;
  delete[] grad_fc3_weights;
  delete[] grad_fc3_biases;
  delete[] momentum_fc3_weights;
  delete[] momentum_fc3_biases;
  
  return 0;
}

